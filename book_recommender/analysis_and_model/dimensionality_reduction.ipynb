{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dimensionality Reduction\n",
    "\n",
    "One way to increase density is by predicting user ratings of books with matrix factorization. In the following section, we will use Single Value Decomposition (SVD) to fill empty ratings by predicting how users would rate books based on similar user reviews. This will entail the following steps:\n",
    "\n",
    "- train/test splits\n",
    "- hyper-parameter tuning for k\n",
    "- SVD\n",
    "\n",
    "In the function below, data is split into training and testing data randomly, using an 90/10 split. Initially, I had considered using a leave-one-user-out approach, but since we are not using content based filtering to recommend books to users, I am not particularly worried about a cold start problem. Instead, I just want to get the most accurate prediction of what each existing user would think of each book. For this purpose, leaving a random subset of ratings out will allow for the highest level of accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_density(mat, name):\n",
    "    '''\n",
    "    Prints the matrix density of the matrix.\n",
    "\n",
    "    Parameters:\n",
    "        mat: a matrix\n",
    "        name: a name for the matrix\n",
    "    '''\n",
    "    n_total = mat.shape[0]*mat.shape[1]\n",
    "    n_ratings = mat.nnz\n",
    "    density = n_ratings/n_total\n",
    "    print(f\"The {name} has a density of: {round(density * 100, 2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['All Ratings Matrix', 'All Normalized Ratings Matrix', 'Filtered Ratings Matrix', 'Filtered Normalized Ratings Matrix']\n",
    "\n",
    "matrices = dict(zip(names, [X, Xn, Xf, Xfn]))\n",
    "\n",
    "for a, b in matrices.items():\n",
    "    matrix_density(b, a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "def split_sparse_matrix_by_ratings(R_sparse, test_size=0.1, random_state=37):\n",
    "    \"\"\"\n",
    "    Split a sparse matrix into train and test sets, keeping a percentage of ratings from each user in the test set.\n",
    "    \n",
    "    Parameters:\n",
    "        R_sparse (csr_matrix): Sparse user-item rating matrix (CSR format).\n",
    "        test_size (float): Fraction of each user's ratings to be used as test set.\n",
    "        random_state (int): Random seed for reproducibility.\n",
    "    \n",
    "    Returns:\n",
    "        R_train (csr_matrix): Train set sparse matrix.\n",
    "        R_test (csr_matrix): Test set sparse matrix.\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # Get user indices and their respective nonzero ratings\n",
    "    R_coo = R_sparse.tocoo()\n",
    "    rows, cols, data = R_coo.row, R_coo.col, R_coo.data\n",
    "    \n",
    "    train_rows, train_cols, train_data = [], [], []\n",
    "    test_rows, test_cols, test_data = [], [], []\n",
    "    \n",
    "    unique_users = np.unique(rows)\n",
    "    \n",
    "    for user in unique_users:\n",
    "        # Get all indices where this user has a rating\n",
    "        user_indices = np.where(rows == user)[0]\n",
    "        \n",
    "        # Shuffle and split userâ€™s ratings\n",
    "        test_indices = np.random.choice(user_indices, size=int(len(user_indices) * test_size), replace=False)\n",
    "        train_indices = np.setdiff1d(user_indices, test_indices)\n",
    "        \n",
    "        # Assign ratings to train and test\n",
    "        train_rows.extend(rows[train_indices])\n",
    "        train_cols.extend(cols[train_indices])\n",
    "        train_data.extend(data[train_indices])\n",
    "        \n",
    "        test_rows.extend(rows[test_indices])\n",
    "        test_cols.extend(cols[test_indices])\n",
    "        test_data.extend(data[test_indices])\n",
    "    \n",
    "    # Create sparse train and test matrices\n",
    "    R_train = sp.csr_matrix((train_data, (train_rows, train_cols)), shape=R_sparse.shape)\n",
    "    R_test = sp.csr_matrix((test_data, (test_rows, test_cols)), shape=R_sparse.shape)\n",
    "    \n",
    "    return R_train, R_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune k \n",
    "\n",
    "ks = [40, 50, 75, 100, 120, 130, 150]\n",
    "\n",
    "scores = []\n",
    "for k in ks:\n",
    "    n_splits = 5\n",
    "    print('Testing k=' + str(k))\n",
    "    rmse = svd_with_cv(X,n_splits, k)\n",
    "    scores.append({k: rmse})\n",
    "    print('Mean RMSE: ' + str(round(sum(rmse) / n_splits, 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only pop books \n",
    "most_pop = ratings[ratings['book_popularity_rated'] > 120]\n",
    "most_pop = most_pop[most_pop['user_books_rated'] > 500]\n",
    "X_popular, user_mapper_pop, book_mapper_pop, user_inv_mapper_pop, book_inv_mapper_pop = create_X(most_pop, user_id='user_id', book_id='book_tag', rating='rating')\n",
    "\n",
    "print(X_popular.shape)\n",
    "\n",
    "matrix_density(X_popular, 'Most Popular Books Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = [1, 2, 4, 10, 15, 20, 30, 40, 50, 55, 60]\n",
    "ks = [9, 10, 11, 12, 13, 14, 15, 16]\n",
    "\n",
    "\n",
    "scores = []\n",
    "for k in ks:\n",
    "    n_splits = 5\n",
    "    print('Testing k=' + str(k))\n",
    "    rmse = svd_with_cv(X_popular,n_splits, k)#reduce_with_svd(R=X, n_splits=1, k=k)\n",
    "    scores.append({k: rmse})\n",
    "    print('Mean RMSE: ' + str(round(sum(rmse) / n_splits, 6)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All ratings, not normalized\n",
    "U, sigma, VT = svds(X_popular, k=12)\n",
    "sigma = np.diag(sigma)\n",
    "\n",
    "# Reconstruct the matrix\n",
    "X_pop_pred = np.dot(np.dot(U, sigma), VT)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
